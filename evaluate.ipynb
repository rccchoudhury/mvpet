{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/rchoudhu/research/voxelpose-pytorch/lib\")\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from tensorboardX import SummaryWriter\n",
    "import argparse\n",
    "import pprint\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "%matplotlib agg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#import _init_paths\n",
    "from core.config import config\n",
    "from core.config import update_config\n",
    "from core.function import train_3d, validate_3d\n",
    "from utils.utils import create_logger\n",
    "from utils.utils import save_checkpoint, load_checkpoint, load_model_state\n",
    "from utils.utils import load_backbone_panoptic\n",
    "from utils.vis import save_debug_3d_images\n",
    "import dataset\n",
    "import models\n",
    "\n",
    "viz_test_dir = \"video_viz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=> load /home/rchoudhu/research/voxelpose-pytorch/data/Shelf/pred_shelf_maskrcnn_hrnet_coco.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> creating /home/rchoudhu/research/voxelpose-pytorch/output/shelf_synthetic/multi_person_posenet_50/prn64_cpn80x80x20\n",
      "=> creating /home/rchoudhu/research/voxelpose-pytorch/log/shelf_synthetic/multi_person_posenet_50/prn64_cpn80x80x202022-02-01-03-24\n",
      "=> Loading data ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchoudhu/research/voxelpose-pytorch/lib/dataset/shelf.py:91: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  actor_3d = np.array(np.array(data['actor3D'].tolist()).tolist()).squeeze()  # num_person * num_frame\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation\n",
      "=> Done loading data.\n"
     ]
    }
   ],
   "source": [
    "cfg = \"configs/shelf/prn64_cpn80x80x20.yaml\"\n",
    "update_config(cfg)\n",
    "logger, final_output_dir, tb_log_dir = create_logger(config, cfg, 'validate')\n",
    "\n",
    "gpus = [int(i) for i in config.GPUS.split(',')]\n",
    "print('=> Loading data ..')\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "print(config.DATASET.TEST_SUBSET)\n",
    "test_dataset = eval('dataset.' + config.DATASET.TEST_DATASET)(\n",
    "    config, config.DATASET.TEST_SUBSET, False,\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "print(test_dataset.image_set)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.TEST.BATCH_SIZE * len(gpus),\n",
    "    shuffle=False,\n",
    "    num_workers=config.WORKERS,\n",
    "        pin_memory=True)\n",
    "\n",
    "print('=> Done loading data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Constructing models ..\n",
      "Done constructing models.\n",
      "Setting data parallel with gpus: [0]\n",
      "Took 2.823 to set up data parallel\n"
     ]
    }
   ],
   "source": [
    "print('=> Constructing models ..')\n",
    "# Set is_train to false\n",
    "model = eval('models.' + config.MODEL + '.get_multi_person_pose_net')(\n",
    "    config, is_train=False)\n",
    "print(\"Done constructing models.\")\n",
    "\n",
    "new_gpus = [i for i in range(len(gpus))]\n",
    "gpus = new_gpus\n",
    "print(\"Setting data parallel with gpus: \" + str(gpus))\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    model = torch.nn.DataParallel(model, device_ids=gpus).cuda()\n",
    "print(\"Took %.3f to set up data parallel\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=> load models state /home/rchoudhu/research/voxelpose-pytorch/output/shelf_synthetic/multi_person_posenet_50/prn64_cpn80x80x20/model_best.pth.tar\n",
      "/home/rchoudhu/.conda/envs/voxelpose_test/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/rchoudhu/research/voxelpose-pytorch/lib/core/proposal.py:19: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  indices_x = (indices // (shape[1] * shape[2])).reshape(batch_size, num_people, -1)\n",
      "/home/rchoudhu/research/voxelpose-pytorch/lib/core/proposal.py:20: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  indices_y = ((indices % (shape[1] * shape[2])) // shape[2]).reshape(batch_size, num_people, -1)\n",
      "Test: [0/76]\tTime: 6.175s (6.175s)\tSpeed: 3.2 samples/s\tData: 4.382s (4.382s)\tMemory 68077056.0\n",
      "Test: [75/76]\tTime: 0.103s (0.863s)\tSpeed: 48.6 samples/s\tData: 0.001s (0.199s)\tMemory 37667840.0\n",
      "/home/rchoudhu/research/voxelpose-pytorch/lib/dataset/shelf.py:169: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  actor_3d = np.array(np.array(data['actor3D'].tolist()).tolist()).squeeze()  # num_person * num_frame\n",
      "     | Actor 1 | Actor 2 | Actor 3 | Average | \n",
      " PCP |  93.33  |  0.00  |  80.00  |  57.78  |\t Recall@500mm: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num person :  4 \n",
      "gt shape : (14, 3)\n",
      "gt shape : (14, 3)\n",
      "num person :  4 \n",
      "gt shape : (14, 3)\n",
      "gt shape : (14, 3)\n",
      "num person :  4 \n",
      "gt shape : (14, 3)\n",
      "gt shape : (14, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5777777775851852"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model_file = os.path.join(final_output_dir, config.TEST.MODEL_FILE)\n",
    "if config.TEST.MODEL_FILE and os.path.isfile(test_model_file):\n",
    "    logger.info('=> load models state {}'.format(test_model_file))\n",
    "    model.module.load_state_dict(torch.load(test_model_file))\n",
    "else:\n",
    "    raise ValueError('Check the model file for testing!')\n",
    "\n",
    "#print(\"Uncomment to actually run the validation, but won't tell us anything we don't know yet\")\n",
    "validate_3d(config, model, test_loader, final_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Viewing/rendering; iterate through data loader. Rewrite the file to a temp output. \n",
    "# then put it together with your normal video script stuff. Final output is a 3d video of the view.\n",
    "# gotta be a matplotlib issue. \n",
    "# shelf / campus (have different skeleton format than the others. )\n",
    "LIMBS14 = [[0, 1], [1, 2], [3, 4], [4, 5], [2, 3], [6, 7], [7, 8], [9, 10],\n",
    "          [10, 11], [2, 8], [3, 9], [8, 12], [9, 12], [12, 13]]\n",
    "\n",
    "#NUM_IMAGES = 30\n",
    "#preds = []\n",
    "#file_name = \"test_viz2.png\"\n",
    "colors = ['r', 'g', 'b', 'black']\n",
    "plot_calls = 0\n",
    "cur_num_person = 0\n",
    "for i, (inputs, target_2d, weights_2d, targets_3d, meta, input_heatmap) in enumerate(test_loader):\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "    #if i > NUM_IMAGES: \n",
    "    #    break\n",
    "    meta = meta[0]\n",
    "    #print(\"viz\")\n",
    "    # Just visualize the ground truth for now. \n",
    "    # weird tensor access...\n",
    "    num_person = meta['num_person'][0].numpy()\n",
    "    if num_person != cur_num_person:\n",
    "        print(\"frame %d : %d \" % (i, num_person))\n",
    "        cur_num_person = num_person\n",
    "\n",
    "    joints_3d = meta['joints_3d'][0]\n",
    "    joints_3d_vis = meta['joints_3d_vis'][0]\n",
    "    #ax = plt.subplot(1, 1, 1, projection='3d')\n",
    "    for n in range(num_person):\n",
    "        #print(n)\n",
    "        joint = joints_3d[n]\n",
    "        joint_vis = joints_3d_vis[n]\n",
    "        #print(joint.shape)\n",
    "        # getting rly cute with the eval usage aren't we\n",
    "        for k in eval(\"LIMBS{}\".format(len(joint))):\n",
    "            #print(\"plotting joint : \" + str(k))\n",
    "            # if joint_vis[k[0], 0] and joint_vis[k[1], 0]:\n",
    "            #     x = [float(joint_vis[k[0], 0]), float(joint_vis[k[1], 0])]\n",
    "            #     y = [float(joint_vis[k[0], 1]), float(joint_vis[k[1], 1])]\n",
    "            #     z = [float(joint_vis[k[0], 2]), float(joint_vis[k[1], 2])]\n",
    "            #     ax.plot(x, y, z, c='g', lw=1.5, marker='o', markerfacecolor='w', markersize=2,\n",
    "            #             markeredgewidth=1)\n",
    "            # else:\n",
    "            x = [float(joint[k[0], 0]), float(joint[k[1], 0])]\n",
    "            y = [float(joint[k[0], 1]), float(joint[k[1], 1])]\n",
    "            z = [float(joint[k[0], 2]), float(joint[k[1], 2])]\n",
    "\n",
    "            ax.plot(x, y, z, c=colors[n], ls='--', lw=1.5, marker='o', markerfacecolor='w', markersize=2,\n",
    "                    markeredgewidth=1)\n",
    "    plt.savefig(os.path.join(viz_test_dir, \"image_%d.png\" % i))\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [00:00<00:00, 147.88it/s]\n"
     ]
    }
   ],
   "source": [
    "image_dir_path = viz_test_dir\n",
    "video_output_path = \"./test.avi\"\n",
    "image_list = sorted(os.listdir(image_dir_path))\n",
    "# Read the first image and check its shape to initialize video \n",
    "# writer.\n",
    "read_img = cv2.imread(os.path.join(image_dir_path, image_list[0]))\n",
    "(height, width, _) = read_img.shape\n",
    "size = (width, height)\n",
    "video_writer = cv2.VideoWriter(video_output_path, cv2.VideoWriter_fourcc(*'DIVX'), 15, size)\n",
    "\n",
    "for image in tqdm(image_list):\n",
    "    img = cv2.imread(os.path.join(image_dir_path, image))\n",
    "    video_writer.write(img)\n",
    "    \n",
    "video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can't re-use their viz stuff, it's useless.\n",
    "# will need to write our own. Key here is getting the right stuff frmo the dataloader\n",
    "# in order. THen once we have the preds, can also try to visualize. the batch thing\n",
    "# is sort of meaningless.....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Well we technically don't need "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3b11fdf824dac3633a9826bd6a6350a128076427a02ff616923448c1271d31e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('voxelpose_test': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
